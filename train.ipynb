{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b261511122c4a37aacdf7aa8c50fe70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "\n",
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame:\n",
      "Dataset({\n",
      "    features: ['0', '1', '2', '3', '__index_level_0__'],\n",
      "    num_rows: 172772\n",
      "})\n",
      "\n",
      "Validation DataFrame:\n",
      "Dataset({\n",
      "    features: ['0', '1', '2', '3', '__index_level_0__'],\n",
      "    num_rows: 43194\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset from the parquet file\n",
    "dataset = pl.read_parquet('hf://datasets/wikimedia/wikipedia/20231101.ko/train-00000-of-00003.parquet')\n",
    "\n",
    "# Convert the Polars DataFrame to a Pandas DataFrame\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Split the data into training and validation sets with an 80:20 ratio\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the Pandas DataFrames back to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# Print the training and validation DataFrames\n",
    "print(\"Train DataFrame:\")\n",
    "print(train_dataset)\n",
    "\n",
    "print(\"\\nValidation DataFrame:\")\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': ['189407',\n",
       "  '363840',\n",
       "  '819321',\n",
       "  '54149',\n",
       "  '492496',\n",
       "  '651045',\n",
       "  '240033',\n",
       "  '829429',\n",
       "  '612374',\n",
       "  '397989'],\n",
       " '1': ['https://ko.wikipedia.org/wiki/%EC%8A%A4%ED%83%A0',\n",
       "  'https://ko.wikipedia.org/wiki/%EC%82%AC%ED%82%A4%EC%9B%90%EC%88%AD%EC%9D%B4',\n",
       "  'https://ko.wikipedia.org/wiki/%ED%8F%B4%20%ED%86%A0%EB%A8%B8%EC%8A%A4',\n",
       "  'https://ko.wikipedia.org/wiki/%EB%85%B8%EB%8D%95%EC%88%A0',\n",
       "  'https://ko.wikipedia.org/wiki/%EC%BF%A8%EB%B7%B0%ED%8B%B0',\n",
       "  'https://ko.wikipedia.org/wiki/%EC%B5%9C%EA%B1%B4%EC%9A%B0',\n",
       "  'https://ko.wikipedia.org/wiki/%EC%84%9C%EC%9A%B8%ED%8A%B9%EB%B3%84%EC%8B%9C%EB%8F%84%20%EC%A0%9C21%ED%98%B8%EC%84%A0%20%28%EC%95%88%EB%82%B4%EC%9A%A9%29',\n",
       "  'https://ko.wikipedia.org/wiki/%EC%98%88%EC%88%98%EC%99%80%20%EA%B0%84%EC%9D%8C%ED%95%9C%20%EC%97%AC%EC%9D%B8',\n",
       "  'https://ko.wikipedia.org/wiki/%EA%B2%BD%ED%83%9C%20%28%EC%97%B0%ED%98%B8%29',\n",
       "  'https://ko.wikipedia.org/wiki/FC%20%EB%8B%A4%ED%82%A4%EC%95%84%20%ED%82%A4%EC%8B%9C%EB%84%88%EC%9A%B0'],\n",
       " '2': ['스탠',\n",
       "  '사키원숭이',\n",
       "  '폴 토머스',\n",
       "  '노덕술',\n",
       "  '쿨뷰티',\n",
       "  '최건우',\n",
       "  '서울특별시도 제21호선 (안내용)',\n",
       "  '예수와 간음한 여인',\n",
       "  '경태 (연호)',\n",
       "  'FC 다키아 키시너우'],\n",
       " '3': ['스탠은 다음을 가리킨다.\\n\\n인명 \\n 스탠 마시(Stan Marsh): 《사우스 파크》 만화영화 시리즈의 중심인물 중 한 명\\n 스탠 코벨레스키(Stan Coveleski)\\n 스탠 게츠(Stan Getz): 미국의 재즈 색소폰 연주가 겸 영화배우\\n 스탠 존스(Stan Jones): 오스트레일리아의 레이싱 선수\\n 스탠 로럴(Stan Laurel): 잉글랜드 출신의 코미디언\\n 스탠 리(Stan Lee): 미국의 만화가, 영화 제작자, 배우\\n 스탠 로저스(Stan Rogers): 캐나다의 싱어송라이터 겸 기타리스트\\n 스탠 뮤지얼(Stan Musial)\\n 스탠 핸슨: 미국의 프로 레슬링 선수, 배우, 영화배우\\n 스탠 시모어: 잉글랜드의 축구 선수이자 감독\\n 스탠 스미스: 전 미국 테니스 선수\\n 스탠 워커: 오스트레일리아에서 태어난 뉴질랜드의 가수이자 배우\\n 스탠 레이: 미국의 극작가 겸 연출가이며 대학 교수\\n 스탠 크롱키: 미국의 기업인이자 스포츠 재벌\\n\\n예술, 미디어, 엔터테인먼트 \\n Stan (노래): 미국의 힙합 가수 에미넴의 3번째 싱글 노래\\n : 스탠 워커의 2018년 EP\\n\\n같이 보기 \\n \\n 스탄 (동음이의)',\n",
       "  '사키원숭이 또는 사키는 사키원숭이속(Pithecia)에 속하는 신세계원숭이의 총칭이다. 수염사키원숭이속(Chiropotes)에 속하는 수염사키원숭이와 밀접한 관계에 있다.\\n\\n분포 \\n\\n사키원숭이의 분포는 남아메리카 북부와 중부를 포함하며, 콜롬비아 북부에서부터 페루를 지나 볼리비아 북부까지 확장된다. 그리고 브라질 중부도 속해 있다.\\n\\n몸의 특징 \\n사키원숭이는 몸집이 작은 원숭이로, 길고 털이 많은 꼬리를 지니고 있다. 부드러운 털이 나고, 거친 이들의 피부는 종에 따라서 검은색이나, 회색 또는 붉은 갈색을 띤다. 일부 종의 얼굴은 털이 덮여있지 않으나, 눈 위로는 털이 덮여 있다. 몸은 나무에서 생활하기에 적합하며, 강한 뒷다리가 있어 멀리 도약도 가능하다. 사키원숭이의 몸 길이는 꼬리 길이와 비슷한 30 ~ 50 cm이고 , 몸무게는 2\\xa0kg 정도이다.\\n\\n습성 \\n사키원숭이는 주행성 동물이다. 우림의 나무에서 생활하고 아주 가끔 땅에 내려 온다. 이들은 주로 팔다리 모두를 써서 움직이며, 가끔은 나뭇가지 위로 뒷다리로 서서 달리기도 하고, 때로는 먼 거리를 뛰어넘기도 한다. 잘 때는 고양이처럼 몸을 구부리고 나무에서 잔다. 이들은 일반적으로 부끄럼을 많이 타며, 경계심이 많은 동물이다\\n\\n생활 \\n사키원숭이는 부모와 새끼로 이루어진 가족 집단 내에서 생활하며, 맺어진 짝은 보통 평생동안 인연을 이어간다. 이들은 다른 가족들로부터 자신의 영역을 보호하는 영역형 동물이다. 사키원숭이는 일군의 커뮤니케이션 수단을 알고 있다: 날카롭고, 새처럼 지저귀는 소리는 가족 구성원들 내에 어떤 연락 역할을 하며, 시끄럽게 고함치는 소리는 자신들의 영역 내에 들어오는 다른 동물에게 경고를 하는 역할을 한다.\\n\\n먹이 \\n사키원숭이는 잡식성 동물이다. 이들은 과일과 나뭇잎, 꽃, 곤충 그리고 설치류나 박쥐와 같은 작은 척추동물을 먹는다.\\n\\n생식 \\n짝짓기 계절은 따로 없으며, 연중 언제라도 가능하다. 임신 기간은 약 150 ~ 180일이며, 한 마리를 낳는다. 4개월이면 젖을 떼고, 완전히 성숙하는 데 3년이 필요하다. 수명은 30년이다.\\n\\n하위 종 \\n 적도사키 (Pithecia aequatorialis)\\n 흰발사키 (Pithecia albicans)\\n 카주자사키 (Pithecia cazuzai)\\n 황금얼굴사키 (Pithecia chrysocephala)\\n 털북숭이사키 (Pithecia hirsuta)\\n 이누스타사키 (Pithecia inusta)\\n 타파조스강사키 (Pithecia irrorata)\\n 이사벨사키 (Pithecia isabela)\\n 밀러수사사키 (Pithecia milleri)\\n 미터마이어타파조스사키 (Pithecia mittermeieri)\\n 수사사키 (Pithecia monachus)\\n 나포사키 (Pithecia napensis)\\n 피시나티흰반점얼굴사키 (Pithecia pissinattii)\\n 흰얼굴사키 (Pithecia pithecia)\\n 라일랜즈흰반점얼굴사키 (Pithecia rylandsi)\\n 반졸리니흰반점얼굴사키 (Pithecia vanzolinii)\\n\\n각주\\n\\n외부 링크 \\n 영장류 정보넷 사키원숭이속(Pithecia) 정보\\n\\n \\n1804년 기재된 포유류',\n",
       "  '폴 토머스(Paul Thomas)라는 이름을 가진 사람은 다음과 같다.\\n\\n 폴 토머스 (포르노 배우)\\n 폴 토머스 (베이스 기타 연주자)',\n",
       "  \"노덕술(盧德述, 1899년 6월 1일 ~ 1968년 4월 1일)은 일제강점기와 대한민국의 경찰이다. 창씨개명 후의 이름은 마쓰우라 히로(松浦 鴻).\\n\\n일제 강점기 고등계 형사이다. 광복과 대한민국 정부 수립 이후 수도경찰청 간부로 재직하였다. 1948년 10월 반민족행위특별조사위원회 및 정부요인 암살 음모 사건의 이종형, 박흥식과 함께 주범 중 한 명이다. 1949년 반민족행위특별조사위원회에 체포됐으나 반민특위 해체로 풀려나 경찰직 복귀 이후 대한민국 경찰직에서 고위간부로 지냈다.\\n\\n생애\\n\\n일제 강점기 \\n1899년 6월 1일 경상남도 울산군 장생포 출신으로 경기도 개성과 한성부를 거쳐 다시 경상남도 울산에서 자랐고 울산보통학교 2년을 다니다 중퇴한 후 일본인이 경영하던 잡화상의 고용인으로 근무하다가 재취직을 위해 일본 제국 홋카이도에서 머물렀다.\\n\\n귀국 후 경찰관에 지원하여 1920년 경남에 있는 순사교습소에 지원하여 6월 경상남도 순사에 임명되었다. 같은 해 9월 순사교습소를 수료한 후, 같은 달 경찰부 보안과 순사로 근무하다가 1922년경 경상남도 울산경찰서 사법계 순사부장으로 재직했다. 1924년 12월 도경부 및 도경부보고시에 합격한 후, 같은 달 경부보로 승진하여 경상남도 의령경찰서 경부보, 1926년 4월 거창경찰서 경부보를 지냈으며, 1927년 12월 동래경찰서 경부보로 전근해 사법주임을 지냈다. 동래경찰서 경부보로 재직 중이던 1928년 10월 동래청년동맹 집행위원장 및 신간회 동래지회 간부로 활동하던 박일형을 체포하여 고문했다. 같은 해 겨울에는 부산제이상업학교 학생들이 주도한 동맹휴교사건을 수사하다가 동맹휴교의 배후에 '혁조회'라는 반일단체가 있음을 알고 혁조회 관련자인 김규직, 유진홍 등을 체포하여 고문했다. 김규직은 고문 후유증으로 1929년 12월 옥사했다. 같은 해 12월 조선공산당사건과 관련하여 동래고등보통학교 학생의 제보를 받고 보통학교 교원을 체포하여 심문했다.\\n\\n1929년 8월 동래유학생학우회 주최로 조선인 일본유학생들이 동래유치원에서 개최한 강연회의 강연 내용이 일본정치를 비난하는 등 내용이 불순하다고 강연자들을 체포하여 심문했으며, 같은 해 12월에는 동래고등보통학교 학생 문재순, 추학, 차일명등이 주도하여 광주학생항일운동 관련자 석방 등을 주장하며 동맹휴학을 일으키자 부하들을 지휘하여 관련자들을 체포하는 한편, 체포된 학생들에게 무자비한 고문을 자행했다. 1931년 경상남도 통영 경찰서 경부보로 전근해 사법주임을 지냈다. 통영경찰서 경부보로 재직 중이던 1932년 5월 노동 운동가 김재학을 '메이데이 시위에 참여했다.'는 혐의로 체포하여 고문했다.\\n\\n1932년 7월 경부로 승진하여 울산경찰서 경부로 전근했다가 같은 해 8월 다시 경기도 경성부 본정경찰서(오늘날 서울 명동) 경부로 옮겨 사업주임을 지냈다. 1933년 2월 인천경찰서 경부, 1934년 2월 양주경찰서 경부, 1938년 11월 개성경찰서 경부로 전근해 사법주임을 지냈다. 양주경찰서와 개성경찰서 경부로 재직 당시 중일 전쟁이 일어나자 군사수송 경계, 여론 환기, 국방사상 보급 선전을 비롯해 조선인의 전쟁협력을 독려하기 위한 각종 시국좌담회에 참석하고 지도하는 등 전시 업무를 적극 수행했다. 이러한 활동은 일제 총독부로부터 공로를 인정받아 1941년 3월 훈8등 서보장을 받았다. 그해 6월 경성 종로경찰서 경부로 전근해 사법주임으로 근무했다.\\n\\n한편 1940년 11월부터 경성 콤그룹에 대한 대규모 검거가 있었다. 체포당한 조직원들은 모두 살인적인 고문 수사를 겪었다. 경성콤그룹 조직원을 고문한 대표적인 경찰이 노덕술이었다. 경성콤그룹 조직원 중 김순원, 김재병, 김덕연 등이 고문치사했다. 노덕술은 경성콤그룹 지도자였던 이관술을 그가 갖고 있는 고문기술을 총동원해서 고문했다.\\n\\n1943년 9월 경시로 승진해 평안남도 경찰부 보안과 경시에 임명되어 보안과장으로 근무했다. 1944년 6월 전시체제하에서 경찰의 임무가 치안유지 이외에 징병, 운송, 방공 등으로 확대되면서 경찰기구가 개편되어 기존의 보안과가 수송보안과로 확대 개편되자 평안남도 경찰부 수송보안과장으로 근무했다. 같은 해 12월에서는 영화와 연극 등의 보급을 통한 사상선도를 목적으로 조직된 '조선흥행협회'이사를 지냈다. 평안남도 경찰부 수송보안과장으로 재직 시 자동차 수송통제를 목적으로 조직된 평남자동차수송협력회의 이사를 지내면서 여러 대의 화물자동차를 징발하여 군수품 수송에 제공하는 등 일본의 전쟁 수행에 적극 협력했다.\\n\\n광복 이후 \\n\\n광복 후, 1945년 8월부터 평안남도 평양경찰서 서장으로 근무하다가 소련군이 진주하면서 체포되어 몇 달간 구금되었다가 풀려났다. 1945년 말에 월남했다. 1946년에 장택상에 의해 수도경찰청 수사과장에 기용되어 경찰 내부의 '반이승만 세력' 숙청, '좌익분자' 검거 등을 주도하였다. 1946년 1월 경기도 경찰부 수사과장에, 9월 제1경무총감부 관방장 겸 수도관구 경찰청 수사과장에 임명되었다. 그해 4월, 당시의 동아일보 사장 송진우를 암살한 한현우 등을 검거함으로써 장택상을 비롯한 경찰 수뇌부들의 인정을 받았다.\\n\\n1946년 학술적으로 조작된 사건으로 판명된 정판사 위조지폐 사건의 피의자들을 고문하였다. 이 때 일제강점기에 노덕술에게 수차례 고문당한 바 있던 독립운동가 이관술이 다시 노덕술에게 고문을 당했다.\\n\\n1948년 7월, 수도경찰청장 장택상 저격하려 했다는 혐의로 붙잡은 박성근을 고문치사 시킨 후 시신을 한강에 투기한 혐의로 경무국 수사국에 체포됐다가 도주했다. 도주 중이던 1948년 10월 수도경찰청 수사지도과장 최난수 등과 함께 아예 반민특위 핵심 관계자 15명의 암살을 모의했다.\\n\\n1949년 1월 24일, 반민특위에 의해 체포되었다. 그는 백민태라는 청부업자를 고용하여 '반민특위 간부들을 암살하라.'고 지시한 음모가 밝혀졌다. 하지만 이틀 뒤 이승만 대통령의 비호를 받아 모두 무죄를 선고받았다.\\n대한민국 대통령 이승만은 '노덕술은 반공투사다. 그를 풀어줘라.'라고 그의 석방을 요구했고, 반민특위는 석방을 거절했으나 얼마 안가 대통령 이승만과 내무차관 장경근의 주도하에 조작된 국회프락치 사건, '6.6 반민특위 습격사건' 등으로 반민특위는 와해되었고, 노덕술은 풀려나게 되어 경기도 경찰부 보안주임으로 영전한다.\\n\\n이후 헌병 중령으로 변신하여 1950년에는 육군 본부에서 범죄수사단장으로 근무하는 등 대공업무를 담당하였으며, 1955년 서울 15범죄수사대 대장을 지냈다. 1955년, 부산 제2육군범죄수사단 대장으로 재임 시의 뇌물수뢰 혐의로 그 해 11월 육군중앙고등군법회의에 회부되어 징역 6개월을 언도받으면서 파면되었다. 이후 1956년 이후 고향 울산으로 내려가 칩거 생활하면서 지내다가 1960년 7월 제5대 국회의원(민의원) 선거때 경상남도 울산에서 무소속으로 출마하였으나 낙선했다. 1968년 4월 1일 사망했다.\\n\\n2002년 발표된 친일파 708인 명단, 2008년 민족문제연구소에서 친일인명사전에 수록하기 위해 정리한 친일인명사전 수록예정자 명단에 선정되었으며 친일반민족행위진상규명위원회가 발표한 친일반민족행위 705인 명단에도 포함되었다.\\n\\n대중문화 \\n 1981년 MBC 《제1공화국》 신충식\\n 1990년 MBC 《반민특위》 신충식\\n 2002년 SBS 《야인시대》 서영탁\\n 2019년 MBC 《이몽》 허성태\\n 고문왕 노덕술의 생애\\n\\n경력 \\n 1920년 경남 순사교습소를 졸업, 경상남도 경찰부 보안과를 시작으로 경찰 근무를 시작하여, 울산경찰서 사법계에 근무하면서 순사부장이 됨.\\n 1924년 경부보로 승진하여 의령, 김해, 거창, 통영 경찰서의 사법주임을 지냈다.\\n 1927년경, 비밀결사조직 혁조회(革潮會) 수사를 담당하였다.\\n 1928년 10월 박일경을 검사, 취조하였다.\\n 1929년 - 1930년 동래경찰서 사법주임으로 재직 당시, 동래고등보통학교 맹휴 사건과 조선인 일본 유학생의 하기휴가 이용 귀국 강연회 사건 수사를 담당하였다.\\n 1932년 5월 통영경찰서 사법주임 재직 시, 독립운동가 김재학(金載學)을 고문 취조\\n 1934년 9월 평안남도 경찰부 보안과장으로 승진\\n 1948년 1월 24일, 임화(任和) 박성근이 수도경찰청장 장택상을 암살하려다 미수하고 체포되었는데, 노덕술과 그 부하 김재곤, 박사일등이 그를 고문하다가 1월 29일 사망케 하였다.\\n 1949년 1월 24일 반민특위 요인 암살 음모 혐의로 체포\\n 1949년 1월 26일 이승만 대통령의 석방 요청, 반민특위의 요청 거절\\n 1950년에는 육군 본부에서 범죄수사단장으로 근무하는 등 대공업무를 담당하였다.\\n 1968년 4월 1일 서울대학교 병원에서 병사했다.\\n\\n역대 선거 결과\\n\\n같이 보기 \\n 친일파\\n 이근안\\n 박처원\\n 정형근\\n 이승만\\n 박흥식 (반민특위 반대한 친일파)\\n 이종형 (반민특위 반대한 친일파)\\n 반민족행위특별조사위원회 및 정부요인 암살 음모 사건\\n\\n각주\\n\\n참고 문헌\\n\\n외부 링크 \\n 친일 고문경찰의 대명사, 노덕술\\n 한국 경찰의 원죄, 노덕술\\n\\n1899년 출생\\n1968년 사망\\n일제강점기의 경찰\\n대한민국의 경찰공무원\\n한국의 군정기\\n대한민국 제1공화국\\n친일파 708인 명단 수록자\\n친일인명사전 수록자\\n울산광역시 출신\\n민주당 (대한민국, 1955년) 당원\\n대한민국 정부 발표 친일반민족행위자\\n대한민국의 반공주의자\\n병사한 사람\\n교하 노씨\\n대한민국 국방부 특별보좌관\\n대한민국 육군 중령\\n대한민국의 한국 전쟁 참전 군인\",\n",
       "  \"쿨뷰티( )는 인터넷 유행어중의 하나로 일본에서 주로 쓰이는 신조어이다.\\n당당하고 시크한 분위기를 가진 냉정침착한 미인이라는 뜻의 칭찬의 말로 한국에서 쓰이는 신조어 '차도녀(차가운 도시 여자)'와 비슷한 의미로 통한다.\\n여성 피겨스케이터로, 토리노 동계올림픽의 금메달리스트인 아라카와 시즈카는 선수시절, 특유의 무표정한 표정으로부터 일찍이 이 별명을 해외언론으로부터 받았었다.\\n\\n각주 \\n\\n일본의 신조어\\n인터넷 신조어\",\n",
       "  '최건우 (본명: 최재웅, 1980년 2월 28일 ~ )는 대한민국의 모델 겸 배우로 활동했다. 소속사는 수엔터테인먼트로 활동했다.\\n\\n출연 작품 \\n 텔레비전 드라마\\n SBS 월화드라마 《자이언트》(2010년 5월 10일 ~ 2010년 12월 7일) ... 이준모 역\\n SBS 월화드라마 《아테나: 전쟁의 여신》(2010년 12월 13일 ~ 2011년 2월 21일) ... 블랙 2 (B2) 요원 역\\n\\n경력 활동 \\n 잡지 에스콰이어\\n 보그\\n 더 스타일\\n 런치박스 모델\\n 패션쇼 에스티듀퐁\\n LG패션\\n\\n수상 \\n 2009년 미스터 월드 코리아 은상\\n\\n1980년 출생\\n대한민국의 남자 배우\\n대한민국의 남자 텔레비전 배우\\n살아있는 사람\\n대한민국의 남자 모델',\n",
       "  '서울특별시도 제21호선은 서울특별시 금천구 시흥동 시흥사거리에서 은평구 대조동 연신내역 사거리에 이르는 서울특별시도로, 도로안내를 위해 부여된 번호이다.\\n\\n설명 \\n이 노선은 시흥사거리에서 시작하여 시흥대로를 쭉 따라가서 강남성심병원이 있는 대림삼거리에서 직진한다. 거기서 여의대방로와 이어져 대방역 삼거리에서 우회전한다. 이후 노량진로를 타고 한강대교를 통과한다. 한강대교 북단에서 이어진 한강대로와 만나고 서울역에 이른다. 서울역삼거리에서 좌회전하여 통일로와 연결되어 연신내역에 이른다. 이 도로의 총 연장은 24.2km이고, 서울특별시도 남북측 간선도로 중에서 맨 첫 번째 노선이다. 전 구간이 구 1번 국도와 중복된다. 통일로는 아시아 고속도로 의 일부에 속한다.\\n\\n구성 \\n 시흥대로 - 여의대방로 - 노량진로 - 한강대로 - 통일로\\n\\n경유지 \\n 서울특별시\\n 금천구 - 관악구 조원동 / 구로구 구로3동 - 동작구 / 영등포구 - 동작구 대방동,  노량진동 - 용산구 -  중구 - 종로구 - 서대문구 - 은평구\\n\\n노선\\n\\n연결 도로 \\n서울특별시도 제21호선은 1번 국도의 흔적이 남아 있기 때문에 남쪽은 안양, 수원으로 이어지는 경수산업도로 (경수대로)로 이어지며, 북쪽은 고양, 파주로 이어지는 통일로와 연결된다.\\n\\n중앙버스전용차로와 BRT계획 \\n현재 이 도로에서 중앙버스전용차로가 시행되고 있는 곳은 시흥사거리에서 대방로의 해군회관 사거리, 노량진로 대방역 삼거리에서 노들역, 한강대로 한강대교 북단 교차로에서 서울역, 통일로에서는 서소문사거리에서 연신내역 등 4곳이다.\\n\\n2015년 현재, 서울역 앞에 서울역 버스 환승센터가 설치되어 있다.\\n\\n시흥대로 남쪽과 연결되는 경수산업도로 중에서 경기도 안양시 동안구 호계동 신(新)사거리에서 시흥대로 시작점까지 이르는 BRT계획이 있다.\\n\\n각주\\n\\n같이 보기 \\n 아시아 고속도로 1호선\\n 국도 제1호선\\n 경수산업도로\\n 통일로\\n\\n서울 금천구의 교통\\n서울 구로구의 교통\\n서울 관악구의 교통\\n서울 영등포구의 교통\\n서울 동작구의 교통\\n서울 용산구의 교통\\n서울 중구의 교통\\n서울 종로구의 교통\\n서울 서대문구의 교통\\n서울 은평구의 교통\\n아시안 하이웨이 1호선',\n",
       "  '예수와 간음한 여인(Pericope Adulteræ)는 요한 복음서 7장 53절에서 8장 11절까지 나오는 일화로 예수님이 간음한 여인을 어떻게 처벌할 지 묻는 바리사이파에게, 죄없는 자가 치라고 하여 사형을 면하게 하고 용서한 일화다. 초기 그리스어 사본에는 없어서 다수의 성서학자는 요한이 기록한 것은 아니라고 본다.\\n\\n배경 \\n모세의 율법은 간음한 여인을 돌로 치라고 규정하였지만, 당시 로마의 지배에 있던 이 지역에서 사형 집행에는 로마 총독의 허가가 필요하였다. 바이사이파들은 이러한 상황을 이용하여 예수를 곤경에 처하게 하려 하였다.\\n\\n각주 \\n\\n신약성경\\n요한의 복음서\\n간통',\n",
       "  '경태(景泰, 1450년 ~ 1457년 1월)는 명나라 대종(代宗) 경태제(景泰帝)의 연호이다. 7년 1개월 가량 사용하였다. 경태제가 탈문의 변으로 인하여 폐위 되면서 태상황 영종이 복위한 후, 경태 연호를 폐하고 천순(天順) 으로 개원하였다.\\n\\n개원 \\n 정통(正統) 14년 9월 6일(1449년 9월 22일)에 연호를 경태(景泰)로 정하고, 내년 1월 1일(1450년 1월 14일)에 개원함.\\n\\n 경태(景泰) 8년 1월 21일(1457년 2월 15일)에 태상황 영종(英宗)의 복위 후, 연호를 천순(天順)으로 개원함.\\n\\n연대 대조표\\n\\n동시대 연호\\n\\n중국 \\n 황소양(黃蕭養) 동양(東陽) (1449년 ~ 1450년)\\n 주휘잡(朱徽煠) 현원(玄元) (1451년 ~ 1452년)\\n 이진(李珍) 천순(天順) (1456년)\\n 왕빈(王斌) 천수(天綉) (1457년)\\n\\n몽골 \\n 오이라트 에센(衛拉特 也先) 첨원(添元) (1453년 ~ 1457년?)\\n\\n베트남 \\n 다이호아(大和) (1443년 ~ 1453년)\\n 지엔닌(延寧) (1454년 ~ 1459년)\\n\\n일본 \\n 호토쿠(寶德) (1449년 ~ 1452년)\\n 교토쿠(享德) (1452년 ~ 1455년)\\n 고쇼(康正) (1455년 ~ 1457년)\\n\\n관련 항목 \\n 중국의 연호\\n\\n각주 \\n\\n명나라의 연호',\n",
       "  'FC 다키아 키시너우(FC Dacia Chişinău)는 몰도바의 수도인 키시너우를 연고지로 하는 축구팀이었다. 1999년에 창단되었고, 2002–03 시즌부터 디비지아 나치오날러에 참가하였다. 2017년 시즌 이후 해체되었다.\\n\\n외부 링크 \\n  공식 홈페이지\\n\\n몰도바의 축구단\\n1999년 설립된 축구단\\n없어진 축구단\\n2017년 해체된 축구단\\n2018년 해체된 축구단'],\n",
       " '__index_level_0__': [58189,\n",
       "  105394,\n",
       "  207449,\n",
       "  18439,\n",
       "  132803,\n",
       "  165362,\n",
       "  71003,\n",
       "  210226,\n",
       "  157014,\n",
       "  113524]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 불러오기\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "import torch\n",
    "\n",
    "# torch_dtype 설정\n",
    "torch_dtype = torch.float16  # 예시로 torch.float16 사용. 필요에 따라 변경 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m quant_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m      2\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      3\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype,\n\u001b[1;32m      5\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle/gemma-2-2b-it\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Projects/KoGemma/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/KoGemma/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3452\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3449\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3452\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3454\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3455\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3456\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[0;32m~/Projects/KoGemma/venv/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:74\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 4-bit quantization requires Accelerate: `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m     )\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_bitsandbytes_available():\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_bnb_backend_availability\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n",
      "\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`"
     ]
    }
   ],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-2b-it\",\n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "              \"google/gemma-2-2b-it\",\n",
    "              trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_params = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,  # epoch는 1로 설정\n",
    "    max_steps=10000,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    warmup_steps=150,  # warmup_steps을 절대값으로 설정\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=100,\n",
    "    push_to_hub=False,\n",
    "    report_to='tensorboard',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kar/Projects/KoGemma/venv/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/kar/Projects/KoGemma/venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/home/kar/Projects/KoGemma/venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e2269ef40d4ec09d5c3e7fee49a0ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/172772 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'question'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize the SFTTrainer\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use the Hugging Face Dataset here\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Use the Hugging Face Dataset here\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpeft_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Specify the appropriate field name\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Additional parameters as needed\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/Projects/KoGemma/venv/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m         message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m custom_message\n\u001b[1;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/KoGemma/venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:361\u001b[0m, in \u001b[0;36mSFTTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs, eval_packing)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m PartialState()\u001b[38;5;241m.\u001b[39mlocal_main_process_first():\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m train_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 361\u001b[0m         train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpacking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_text_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m            \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_of_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchars_per_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_unused_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove_unused_columns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    374\u001b[0m         _multiple \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(eval_dataset, \u001b[38;5;28mdict\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/KoGemma/venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:514\u001b[0m, in \u001b[0;36mSFTTrainer._prepare_dataset\u001b[0;34m(self, dataset, tokenizer, packing, dataset_text_field, max_seq_length, formatting_func, num_of_sequences, chars_per_token, remove_unused_columns, append_concat_token, add_special_tokens, skip_prepare_dataset)\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    509\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to provide either `dataset_text_field` or `formatting_func` argument. Alternatively, you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan skip the dataset preparation by using `SFTConfig(dataset_kwargs=\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskip_prepare_dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: True})`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    511\u001b[0m     )\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m packing:\n\u001b[0;32m--> 514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_non_packed_dataloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremove_unused_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_packed_dataloader(\n\u001b[1;32m    526\u001b[0m         tokenizer,\n\u001b[1;32m    527\u001b[0m         dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    534\u001b[0m         add_special_tokens,\n\u001b[1;32m    535\u001b[0m     )\n",
      "File \u001b[0;32m~/Projects/KoGemma/venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:588\u001b[0m, in \u001b[0;36mSFTTrainer._prepare_non_packed_dataloader\u001b[0;34m(self, tokenizer, dataset, dataset_text_field, max_seq_length, formatting_func, add_special_tokens, remove_unused_columns)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, datasets\u001b[38;5;241m.\u001b[39mDataset):\n\u001b[1;32m    587\u001b[0m     map_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_num_proc  \u001b[38;5;66;03m# this arg is not available for IterableDataset\u001b[39;00m\n\u001b[0;32m--> 588\u001b[0m tokenized_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmap_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_dataset\n",
      "File \u001b[0;32m~/Projects/KoGemma/venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:578\u001b[0m, in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m         dataset\u001b[38;5;241m.\u001b[39mset_format(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_format)\n\u001b[1;32m    577\u001b[0m         dataset\u001b[38;5;241m.\u001b[39m_fingerprint \u001b[38;5;241m=\u001b[39m fingerprint\n\u001b[0;32m--> 578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Projects/KoGemma/venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:543\u001b[0m, in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/Projects/KoGemma/venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3073\u001b[0m, in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3070\u001b[0m     validate_fingerprint(new_fingerprint)\n\u001b[1;32m   3071\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_fingerprint\n\u001b[0;32m-> 3073\u001b[0m prev_env \u001b[38;5;241m=\u001b[39m deepcopy(os\u001b[38;5;241m.\u001b[39menviron)\n\u001b[1;32m   3074\u001b[0m \u001b[38;5;66;03m# check if parallelism if off\u001b[39;00m\n\u001b[1;32m   3075\u001b[0m \u001b[38;5;66;03m# from https://github.com/huggingface/tokenizers/blob/bb668bc439dc34389b71dbb8ce0c597f15707b53/tokenizers/src/utils/parallelism.rs#L22\u001b[39;00m\n\u001b[1;32m   3076\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prev_env\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTOKENIZERS_PARALLELISM\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   3077\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3078\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3083\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3084\u001b[0m ):\n",
      "File \u001b[0;32m~/Projects/KoGemma/venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3449\u001b[0m, in \u001b[0;36m_map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3446\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3447\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[0;32m-> 3449\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   3450\u001b[0m         buf_writer, writer, tmp_file \u001b[38;5;241m=\u001b[39m init_buffer_and_writer()\n\u001b[1;32m   3451\u001b[0m         stack\u001b[38;5;241m.\u001b[39menter_context(writer)\n",
      "File \u001b[0;32m~/Projects/KoGemma/venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3330\u001b[0m, in \u001b[0;36mapply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3327\u001b[0m     inputs_to_merge = inputs\n\u001b[1;32m   3328\u001b[0m if remove_columns is not None:\n\u001b[1;32m   3329\u001b[0m     for column in remove_columns:\n\u001b[0;32m-> 3330\u001b[0m         # `function` can modify input in-place causing column to be already removed.\n\u001b[1;32m   3331\u001b[0m         if column in inputs_to_merge:\n\u001b[1;32m   3332\u001b[0m             inputs_to_merge.pop(column)\n",
      "File \u001b[0;32m~/Projects/KoGemma/venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:552\u001b[0m, in \u001b[0;36mSFTTrainer._prepare_non_packed_dataloader.<locals>.tokenize\u001b[0;34m(element)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(element):\n\u001b[1;32m    551\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[0;32m--> 552\u001b[0m         \u001b[43melement\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_formatting_func \u001b[38;5;28;01melse\u001b[39;00m formatting_func(element),\n\u001b[1;32m    553\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m    554\u001b[0m         truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    555\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    556\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_seq_length,\n\u001b[1;32m    557\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    558\u001b[0m         return_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    559\u001b[0m     )\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_formatting_func \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formatting_func(element), \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    563\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `formatting_func` should return a list of processed strings since it can lead to silent bugs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    564\u001b[0m         )\n",
      "File \u001b[0;32m~/Projects/KoGemma/venv/lib/python3.10/site-packages/datasets/formatting/formatting.py:270\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpa_table \u001b[38;5;241m=\u001b[39m pa_table\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformatter \u001b[38;5;241m=\u001b[39m formatter\n\u001b[0;32m--> 270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {key: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m pa_table\u001b[38;5;241m.\u001b[39mcolumn_names}\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys_to_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[0;31mKeyError\u001b[0m: 'question'"
     ]
    }
   ],
   "source": [
    "# Initialize the SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_params,\n",
    "    train_dataset=train_dataset,  # Use the Hugging Face Dataset here\n",
    "    eval_dataset=val_dataset,      # Use the Hugging Face Dataset here\n",
    "    peft_config=peft_params,\n",
    "    dataset_text_field=\"question\",  # Specify the appropriate field name\n",
    "    # Additional parameters as needed\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 체크포인트 경로\n",
    "checkpoint_path = \"./results/checkpoint-10000\"\n",
    "\n",
    "# 모델 로드\n",
    "model = YourModelClass.from_pretrained(checkpoint_path)\n",
    "\n",
    "# SFTTrainer 설정 (동일하게 유지)\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    peft_config=peft_params,\n",
    "    dataset_text_field=\"question\",\n",
    "    # 필요한 추가적인 파라미터들\n",
    ")\n",
    "\n",
    "# 학습 재개\n",
    "trainer.train(resume_from_checkpoint=checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('./models/20240704')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
